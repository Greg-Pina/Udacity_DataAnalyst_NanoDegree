# ğŸµ Data Modeling with Postgres - Sparkify Project

## ğŸ“Œ Introduction
### What is Sparkify?
Sparkify is a **music streaming startup** looking to **analyze user activity** and **song preferences** to improve recommendations and business strategy.  
The company collects **log data** on user interactions and song metadata from their app.

### Purpose of This Project
The goal of this project is to:
âœ… Design an **optimized Postgres database schema** for **analytics**  
âœ… Build an **ETL pipeline** that extracts **JSON log & song data**, transforms it, and loads it into the database  
âœ… Enable the Sparkify team to run **SQL queries** to analyze user behavior and song preferences  

---

## ğŸ—ï¸ Database Schema and ETL Process

### **ğŸ“Š Database Schema: Star Schema**
We use a **Star Schema** to optimize analytical queries.  
- **Fact Table:**
  - `songplays` (contains song play events)
- **Dimension Tables:**
  - `users` (details about users)
  - `songs` (details about songs)
  - `artists` (details about artists)
  - `time` (timestamps broken into components)

#### **Schema Diagram**
```
          songplays (fact)
        /         |        \
  users          songs     artists
        \         |        /
                time
```

### **ğŸ”„ ETL Pipeline**
1. **Process `song_data`**:
   - Extracts song and artist details from JSON files
   - Loads data into **`songs`** and **`artists`** tables
2. **Process `log_data`**:
   - Filters for `NextSong` events
   - Extracts and loads:
     - **Timestamps** into the `time` table
     - **User details** into the `users` table
     - **Song plays** into the `songplays` table (by looking up song/artist ID)

---

## ğŸ“‚ Files in Repository

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ song_data/   # JSON files with song metadata
â”‚   â”œâ”€â”€ log_data/    # JSON files with user activity logs
â”œâ”€â”€ sql_queries.py   # SQL queries for table creation, inserts, and lookups
â”œâ”€â”€ create_tables.py # Script to set up (drop & create) the database tables
â”œâ”€â”€ etl.py           # Main ETL script to extract, transform, and load data
â”œâ”€â”€ test.ipynb       # Jupyter Notebook to validate data insertion
â”œâ”€â”€ README.md        # Project documentation (THIS FILE)
```

---

## ğŸš€ How to Run the Python Scripts

### **1ï¸âƒ£ Set Up the Database**
First, create the tables by running:
```bash
python3 create_tables.py
```
âœ… **This will drop & recreate all tables in the `sparkifydb` database.**

---

### **2ï¸âƒ£ Run the ETL Pipeline**
To process all files and load the data into Postgres, run:
```bash
python3 etl.py
```
âœ… **This will extract, transform, and load the data into the database.**

---

### **3ï¸âƒ£ Verify Data Insertion**
Open **test.ipynb** and run the queries to verify the tables contain data.

Example SQL query to check songplays:
```sql
SELECT * FROM songplays LIMIT 5;
```

---

## ğŸ† Key Features of This Project
âœ… **PostgreSQL database schema optimized for analytics**  
âœ… **Python-based ETL pipeline for batch processing JSON data**  
âœ… **Star schema design for faster queries**  
âœ… **Handles missing data & duplicates gracefully**  
âœ… **Uses SQL joins for song-artist lookups in `songplays`**  

---

## ğŸ“œ Additional Notes
- This project demonstrates skills in **database modeling, ETL pipelines, SQL, and Python scripting**.
- The ETL script is modular and **scalable** to process large datasets.
- Proper **error handling** ensures smooth data ingestion.

ğŸš€ **Built by [Greg Pina] for Udacityâ€™s Data Engineering Nanodegree!**
