{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import subprocess\n",
    "import zipfile\n",
    "import  yfinance as yf\n",
    "from typing import List, Dict\n",
    "import beaapi as bea\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "load_dotenv()\n",
    "current_date = datetime.now().strftime('%m-%d-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ssga_excel(etf: str, subfolder: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the SSGA Excel file for a single ETF symbol.\n",
    "    \n",
    "    Parameters:\n",
    "    etf (str): ETF symbol in lowercase (e.g., \"xli\", \"xlk\").\n",
    "    subfolder (str): Directory in which to save the downloaded file.\n",
    "    \n",
    "    Returns:\n",
    "    str: Path to the downloaded Excel file.\n",
    "    \"\"\"\n",
    "    url = f'https://www.ssga.com/library-content/products/fund-data/etfs/us/holdings-daily-us-en-{etf}.xlsx'\n",
    "    response = req.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Failed to download {etf}. HTTP status code: {response.status_code}\")\n",
    "    \n",
    "    current_date = datetime.now().strftime('%m-%d-%Y')\n",
    "    file_path = os.path.join(subfolder, f'{etf}-{current_date}.xlsx')\n",
    "    \n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "def process_ssga_excel(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the ETF Excel file from SSGA, trims disclaimers, removes empty lines, and returns a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Local path to the downloaded Excel file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Processed SSGA holdings data with columns [Name, Ticker, Identifier, SEDOL, Weight, Sector, ...].\n",
    "    \"\"\"\n",
    "    # The file has disclaimers in the last rows, skiprows=4 means we skip the top disclaimers\n",
    "    # usecols=\"A:H\" to read only the first 8 columns\n",
    "    df = pd.read_excel(file_path, skiprows=4, header=0, usecols=\"A:H\")\n",
    "    \n",
    "    # If the disclaimers line is found, slice the DataFrame up to that row\n",
    "    disclaimers_str = (\n",
    "        \"Past performance is not a reliable indicator of future performance. \"\n",
    "        \"Investment return and principal value will fluctuate, so you may have a gain or loss when shares are sold. \"\n",
    "        \"Current performance may be higher or lower than that quoted. All results are historical and assume the \"\n",
    "        \"reinvestment of dividends and capital gains. Visit www.ssga.com for most recent month-end performance. \"\n",
    "    )\n",
    "    drop_index = df[df['Name'] == disclaimers_str].index\n",
    "    if not drop_index.empty:\n",
    "        df = df[:drop_index[0]]\n",
    "    \n",
    "    # Remove last row if it's entirely NaN\n",
    "    if not df.empty and df.iloc[-1].isna().all():\n",
    "        df = df[:-1]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fetch_and_process_etf_data(etf_list):\n",
    "    \"\"\"\n",
    "    Download and parse SSGA Excel holdings for a list of ETF tickers.\n",
    "    \n",
    "    Parameters:\n",
    "    etf_list (List[str]): e.g. ['XLI', 'XLK', 'XLE', 'XLB'] (uppercase or mixed case).\n",
    "    \n",
    "    Returns:\n",
    "    List[pd.DataFrame]: A list of DataFrames, one for each ETF, in the same order as etf_list.\n",
    "    \"\"\"\n",
    "    subfolder = 'SSGA Data'\n",
    "    if not os.path.exists(subfolder):\n",
    "        os.makedirs(subfolder)\n",
    "\n",
    "    ssga_df_list = []\n",
    "    \n",
    "    for etf_symbol in etf_list:\n",
    "        etf_lower = etf_symbol.lower()\n",
    "        # 1) Download the Excel\n",
    "        file_path = fetch_ssga_excel(etf_lower, subfolder)\n",
    "        # 2) Process / clean up the DataFrame\n",
    "        df = process_ssga_excel(file_path)\n",
    "        df['ETF'] = etf_symbol.upper()\n",
    "        ssga_df_list.append(df)\n",
    "\n",
    "    return ssga_df_list\n",
    "\n",
    "etf_list = ['XLI', 'XLK', 'XLE', 'XLB']\n",
    "ssga_df_list = fetch_and_process_etf_data(etf_list)\n",
    "\n",
    "# Print tails to verify we have data for each\n",
    "for idx, df in enumerate(ssga_df_list, start=1):\n",
    "    print(f\"\\n=== SSGA DataFrame #{idx} (ETF: {etf_list[idx-1]}) ===\")\n",
    "    print(df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_bea_json(api_key: str, years: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches GDP by Industry data from the BEA API in JSON format.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    api_key : str\n",
    "        our BEA API key.\n",
    "    years : str\n",
    "        A comma-separated string of years (e.g., \"2020,2021,2022,2023,2024\").\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Parsed JSON response from the BEA API, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    base_url = \"https://apps.bea.gov/api/data/\"\n",
    "    params = {\n",
    "        \"UserID\": api_key,\n",
    "        \"method\": \"GetData\",\n",
    "        \"datasetname\": \"GDPbyIndustry\",\n",
    "        \"Frequency\": \"Q\",\n",
    "        \"Year\": years,\n",
    "        \"Industry\": \"ALL\",\n",
    "        \"TableID\": \"ALL\",\n",
    "        \"ResultFormat\": \"JSON\"\n",
    "    }\n",
    "    try:\n",
    "        response = req.get(base_url, params=params)\n",
    "        response.raise_for_status()  # raises HTTPError if status != 200\n",
    "        data = response.json()\n",
    "        \n",
    "        # Check for error key in the response\n",
    "        if 'Error' in data.get('BEAAPI', {}):\n",
    "            err = data['BEAAPI']['Error']\n",
    "            print(f\"API request failed with code {err['APIErrorCode']}: {err['APIErrorDescription']}\")\n",
    "            return None\n",
    "        \n",
    "        return data\n",
    "    except req.exceptions.RequestException as e:\n",
    "        print(f\"API request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_bea_json(raw_data: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parses the raw JSON from BEA into a pivoted DataFrame, excluding certain irrelevant rows.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    raw_data : dict\n",
    "        The JSON dictionary returned by fetch_bea_json.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A pivoted DataFrame with columns for YearQuarter, and rows for each Industry/IndustryDescription.\n",
    "        Certain 'unwanted' industry descriptions are filtered out.\n",
    "        Returns None if the data is missing or invalid.\n",
    "    \"\"\"\n",
    "    if not raw_data or 'BEAAPI' not in raw_data or 'Results' not in raw_data['BEAAPI']:\n",
    "        print(\"Error: 'Results' key not found in the API response.\")\n",
    "        return None\n",
    "\n",
    "    results = raw_data['BEAAPI']['Results']\n",
    "    df_results = pd.DataFrame(results)\n",
    "    if df_results.empty or 'Data' not in df_results.columns:\n",
    "        print(\"Error: No 'Data' found in the Results.\")\n",
    "        return None\n",
    "\n",
    "    # 'Data' itself is in a list of dictionaries\n",
    "    nested_data = df_results.loc[0, \"Data\"]\n",
    "    bea_df = pd.DataFrame(nested_data)\n",
    "    \n",
    "    # Fix possible typos in the column name\n",
    "    if \"IndustrYDescription\" in bea_df.columns:\n",
    "        bea_df.rename(columns={\"IndustrYDescription\": \"IndustryDescription\"}, inplace=True)\n",
    "\n",
    "    # Convert DataValue to float\n",
    "    bea_df['DataValue'] = bea_df['DataValue'].astype(float, errors='raise')\n",
    "\n",
    "    # Create a YearQuarter column to pivot on\n",
    "    bea_df['YearQuarter'] = bea_df['Year'].astype(str) + 'Q' + bea_df['Quarter']\n",
    "\n",
    "    # Pivot: index by (Industry, IndustryDescription), columns=YearQuarter, values=DataValue\n",
    "    pivot_df = bea_df.pivot_table(\n",
    "        index=['Industry', 'IndustryDescription'],\n",
    "        columns='YearQuarter',\n",
    "        values='DataValue',\n",
    "        aggfunc='sum'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Filter out unwanted industry descriptions\n",
    "    excluded_industries = [\n",
    "        \"Taxes on production and imports less subsidies\",\n",
    "        \"Energy inputs\",\n",
    "        \"Intermediate inputs\",\n",
    "        \"Materials inputs\",\n",
    "        \"Purchased-services inputs\",\n",
    "        \"Value added\",\n",
    "        \"Compensation of employees\",\n",
    "        \"Gross operating surplus\"\n",
    "    ]\n",
    "    filtered_df = pivot_df[~pivot_df['IndustryDescription'].isin(excluded_industries)]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def save_bea_raw_data(df: pd.DataFrame, folder: str = 'BEA Data') -> str:\n",
    "    \"\"\"\n",
    "    Saves the DataFrame to an Excel file with a timestamp-based filename.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The pivoted and filtered DataFrame from process_bea_json.\n",
    "    folder : str\n",
    "        Destination folder name.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str : Path to the saved Excel file.\n",
    "    \"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    current_date = datetime.now().strftime('%m-%d-%Y')\n",
    "    file_path = os.path.join(folder, f\"bea-gdp-by-industry-raw-{current_date}.xlsx\")\n",
    "    df.to_excel(file_path, index=False)\n",
    "    return file_path\n",
    "\n",
    "def map_sectors(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Maps each IndustryDescription to a top-level sector (Technology, Materials, Energy, Industrials),\n",
    "    returning only rows in those focus sectors.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Pivoted DataFrame with columns [Industry, IndustryDescription, ...quarter columns... ].\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A copy of df with a new 'Sector' column. Rows outside the focus sectors are excluded.\n",
    "    \"\"\"\n",
    "    sector_map = {\n",
    "        \"Technology\": [\n",
    "            \"Computer and electronic products\",\n",
    "            \"Computer systems design and related services\",\n",
    "            \"Data processing, internet publishing, and other information services\",\n",
    "            \"Information-communications-technology-producing industries\"\n",
    "        ],\n",
    "        \"Materials\": [\n",
    "            \"Agriculture, forestry, fishing, and hunting\",\n",
    "            \"Farms\",\n",
    "            \"Forestry, fishing, and related activities\",\n",
    "            \"Mining\",\n",
    "            \"Mining, except oil and gas\",\n",
    "            \"Support activities for mining\",\n",
    "            \"Wood products\",\n",
    "            \"Paper products\",\n",
    "            \"Chemical products\",\n",
    "            \"Plastics and rubber products\",\n",
    "            \"Nonmetallic mineral products\",\n",
    "            \"Primary metals\",\n",
    "            \"Fabricated metal products\"\n",
    "        ],\n",
    "        \"Energy\": [\n",
    "            \"Oil and gas extraction\",\n",
    "            \"Petroleum and coal products\",\n",
    "            \"Pipeline transportation\"\n",
    "        ],\n",
    "        \"Industrials\": [\n",
    "            \"Construction\",\n",
    "            \"Machinery\",\n",
    "            \"Electrical equipment, appliances, and components\",\n",
    "            \"Other transportation equipment\",\n",
    "            \"Miscellaneous manufacturing\",\n",
    "            \"Durable goods\",\n",
    "            \"Wholesale trade\",\n",
    "            \"Rail transportation\",\n",
    "            \"Water transportation\",\n",
    "            \"Truck transportation\",\n",
    "            \"Transit and ground passenger transportation\",\n",
    "            \"Other transportation and support activities\",\n",
    "            \"Transportation and warehousing\",\n",
    "            \"Warehousing and storage\",\n",
    "            \"Waste management and remediation services\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Helper to assign a top-level sector\n",
    "    def get_sector(description: str) -> str:\n",
    "        for sector, cat_list in sector_map.items():\n",
    "            if description in cat_list:\n",
    "                return sector\n",
    "        return \"Other\"\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"Sector\"] = df_copy[\"IndustryDescription\"].apply(get_sector)\n",
    "\n",
    "    focus_sectors = [\"Technology\", \"Materials\", \"Energy\", \"Industrials\"]\n",
    "    df_filtered = df_copy[df_copy[\"Sector\"].isin(focus_sectors)]\n",
    "    return df_filtered\n",
    "\n",
    "def fetch_bea_gdp_by_industry(api_key: str, years: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    High-level orchestrator that fetches BEA data, processes it, saves a raw Excel,\n",
    "    maps top-level sectors, and saves a final filtered Excel.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    api_key : str\n",
    "        our BEA API key.\n",
    "    years : str\n",
    "        Comma-separated years (e.g. \"2020,2021,2022,2023,2024\").\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        The final DataFrame with top-level sectors included.\n",
    "    \"\"\"\n",
    "    raw_json = fetch_bea_json(api_key, years)\n",
    "    if raw_json is None:\n",
    "        return None\n",
    "    \n",
    "    pivoted_df = process_bea_json(raw_json)\n",
    "    if pivoted_df is None:\n",
    "        print(\"Error: Could not process BEA data into pivoted DataFrame.\")\n",
    "        return None\n",
    "\n",
    "    # Save the pivoted \"raw\" data\n",
    "    _ = save_bea_raw_data(pivoted_df)  # optional to keep the file path\n",
    "\n",
    "    # Map each IndustryDescription to a top-level sector\n",
    "    final_df = map_sectors(pivoted_df)\n",
    "\n",
    "    # Save a separate 'filtered' file\n",
    "    subfolder = 'BEA Data'\n",
    "    current_date = datetime.now().strftime('%m-%d-%Y')\n",
    "    filtered_path = os.path.join(subfolder, f\"bea-gdp-by-industry-filtered-{current_date}.xlsx\")\n",
    "    final_df.to_excel(filtered_path, index=False)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    api_key = os.environ.get(\"beakey\")  # our key as a string\n",
    "    years = \"2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024\"\n",
    "    \n",
    "    bea_df = fetch_bea_gdp_by_industry(api_key, years)\n",
    "    if bea_df is not None:\n",
    "        print(bea_df.tail())\n",
    "    else:\n",
    "        print(\"BEA GDP data retrieval or processing failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_kaggle_data():\n",
    "    # Step 1: Ensure the \"Kaggle Data\" directory exists\n",
    "    kaggle_data_dir = \"Kaggle Data\"\n",
    "    os.makedirs(kaggle_data_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 2: Check if the .csv file already exists\n",
    "    csv_exists = any(filename.endswith('.csv') for filename in os.listdir(kaggle_data_dir))\n",
    "    \n",
    "    if not csv_exists:\n",
    "        # Step 3: Download the dataset using the Kaggle CLI\n",
    "        dataset = \"jakewright/9000-tickers-of-stock-market-data-full-history\"\n",
    "        subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", dataset])\n",
    "        \n",
    "        # Step 4: Unzip the downloaded file\n",
    "        zip_filename = dataset.split('/')[-1] + \".zip\"\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall(kaggle_data_dir)\n",
    "        \n",
    "        # Step 5: Remove any files in the \"Kaggle Data\" directory that are not .csv files\n",
    "        for filename in os.listdir(kaggle_data_dir):\n",
    "            if not filename.endswith('.csv'):\n",
    "                os.remove(os.path.join(kaggle_data_dir, filename))\n",
    "        \n",
    "        # Remove the downloaded zip file\n",
    "        os.remove(zip_filename)\n",
    "\n",
    "# Call the function\n",
    "setup_kaggle_data()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "kag_df = pd.read_csv('Kaggle Data/all_stock_data.csv')\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "kag_df['Date'] = pd.to_datetime(kag_df['Date'])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "kag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for dates between 2010-01-01 and 2024-12-31\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2024-12-31'\n",
    "filtered_kag_df = kag_df[(kag_df['Date'] >= start_date) & (kag_df['Date'] <= end_date)]\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame\n",
    "filtered_kag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_directory_exists(directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Ensure the specified directory exists, creating it if necessary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : str\n",
    "        The path of the directory to check or create.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory '{directory}' created.\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "def download_data(tickers: List[str], start_date: str, end_date: str, interval: str = \"1d\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download historical stock data for multiple tickers using yfinance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tickers : List[str]\n",
    "        A list of ticker symbols (e.g. [\"XLI\",\"XLK\",\"XLE\"]).\n",
    "    start_date : str\n",
    "        Start date in \"YYYY-MM-DD\" format.\n",
    "    end_date : str\n",
    "        End date in \"YYYY-MM-DD\" format.\n",
    "    interval : str\n",
    "        Data interval (e.g., \"1d\",\"1wk\",\"1mo\",\"3mo\"). Defaults to \"1d\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A multi-index DataFrame with columns as (Ticker, Field).\n",
    "        By default, yfinance includes columns like \"Open\",\"High\",\"Low\",\"Close\",\"Volume\".\n",
    "    \"\"\"\n",
    "    return yf.download(\n",
    "        tickers=tickers,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        interval=interval,\n",
    "        group_by=\"ticker\",\n",
    "        auto_adjust=True,\n",
    "        threads=True,\n",
    "        progress=True\n",
    "    )\n",
    "\n",
    "def filter_columns(df: pd.DataFrame, tickers: List[str], fields: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select specific columns (fields) for each ticker from a multi-index DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The multi-index DataFrame returned by download_data.\n",
    "    tickers : List[str]\n",
    "        The list of tickers to keep columns for.\n",
    "    fields : List[str]\n",
    "        The list of fields to keep for each ticker (e.g. [\"High\",\"Low\"]).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A subset of the original DataFrame containing only the specified (Ticker, Field) columns.\n",
    "    \"\"\"\n",
    "    # Build a list of valid (Ticker, Field) column tuples\n",
    "    keep_cols = [(t, f) for t in tickers for f in fields if (t, f) in df.columns]\n",
    "    return df[keep_cols].copy()\n",
    "\n",
    "def add_change_columns(df: pd.DataFrame, tickers: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a \"Change\" column for each ticker, computed as (High - Low).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        A multi-index DataFrame with columns like (Ticker, \"High\"), (Ticker, \"Low\").\n",
    "    tickers : List[str]\n",
    "        List of tickers to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Modified DataFrame with additional (Ticker, \"Change\") columns for each ticker having \"High\" and \"Low\".\n",
    "    \"\"\"\n",
    "    for t in tickers:\n",
    "        if (t, \"High\") in df.columns and (t, \"Low\") in df.columns:\n",
    "            df[(t, \"Change\")] = df[(t, \"High\")] - df[(t, \"Low\")]\n",
    "    return df\n",
    "\n",
    "def process_change_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Restructure 'Change' columns from a multi-index to a single-level index.\n",
    "\n",
    "    This extracts all (Ticker, \"Change\") columns, flattens them to 'Ticker_Change' columns,\n",
    "    and resets the index so there is a single 'Date' column instead of the multi-index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns like (Ticker, \"Change\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A new DataFrame with each ticker's change in a single column named 'Ticker_Change',\n",
    "        plus a 'Date' column (extracted from the index).\n",
    "    \"\"\"\n",
    "    # Filter just the \"Change\" columns\n",
    "    df_only_change = df.loc[:, (slice(None), \"Change\")].copy()  # multi-index slice\n",
    "    # Remove the second level from column names\n",
    "    df_only_change.columns = df_only_change.columns.droplevel(1)\n",
    "    # Rename columns => Ticker_Change\n",
    "    df_only_change.columns = [f\"{ticker}_Change\" for ticker in df_only_change.columns]\n",
    "    df_only_change.reset_index(inplace=True)\n",
    "    \n",
    "    # Index column is named \"index\", renaming it to \"Date\"\n",
    "    if df_only_change.columns[0] == \"index\":\n",
    "        df_only_change.rename(columns={\"index\": \"Date\"}, inplace=True)\n",
    "    return df_only_change\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save a DataFrame to CSV with no index, and print confirmation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame to save.\n",
    "    path : str\n",
    "        Output CSV file path.\n",
    "    \"\"\"\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Data with only 'Change' columns saved to: {path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"Yahoo Data\"\n",
    "    ensure_directory_exists(data_dir)\n",
    "\n",
    "    tickers = [\"XLI\", \"XLK\", \"XLE\", \"XLB\"]\n",
    "    # 1) Download data from yfinance\n",
    "    df_full = download_data(tickers, start_date=\"2010-01-01\", end_date=\"2024-01-01\", interval=\"3mo\")\n",
    "    \n",
    "    # 2) Filter columns to just 'High' and 'Low'\n",
    "    df_filtered = filter_columns(df_full, tickers, [\"High\", \"Low\"])\n",
    "    \n",
    "    # 3) Add (Ticker,\"Change\") columns\n",
    "    df_filtered = add_change_columns(df_filtered, tickers)\n",
    "    \n",
    "    # 4) Extract only the \"Change\" columns into a single-level DataFrame\n",
    "    df_only_change = process_change_columns(df_filtered)\n",
    "    \n",
    "    # 5) Save the final result\n",
    "    csv_path = os.path.join(data_dir, \"sector_quarterly_only_change.csv\")\n",
    "    save_to_csv(df_only_change, csv_path)\n",
    "\n",
    "    print(df_only_change.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_tickers_from_ssga(\n",
    "    folder_path: str,\n",
    "    n: int = 10,\n",
    "    skiprows: int = 4,\n",
    "    weight_col: str = \"Weight\",\n",
    "    ticker_col: str = \"Ticker\"\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Read all SSGA Excel files in a folder, extract the top n tickers by weight, and return a unique list of them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        Path to the folder containing .xlsx files from SSGA.\n",
    "    n : int\n",
    "        Number of top holdings to extract per file. Default is 10.\n",
    "    skiprows : int\n",
    "        Rows to skip at the top when reading the Excel. Default 4 based on our file format.\n",
    "    weight_col : str\n",
    "        Column name for weight in the Excel file. Default is \"Weight\".\n",
    "    ticker_col : str\n",
    "        Column name for ticker in the Excel file. Default is \"Ticker\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        A unique list of top ticker symbols across all SSGA Excel files in the folder.\n",
    "    \"\"\"\n",
    "    ssga_files = glob.glob(os.path.join(folder_path, '*.xlsx'))\n",
    "    top_tickers = []\n",
    "\n",
    "    for file in ssga_files:\n",
    "        df = pd.read_excel(file, skiprows=skiprows)\n",
    "        # Grab top n rows by weight, extract ticker col\n",
    "        top_n = df.nlargest(n, weight_col)[ticker_col].tolist()\n",
    "        top_tickers.extend(top_n)\n",
    "\n",
    "    # Deduplicate\n",
    "    return list(set(top_tickers))\n",
    "\n",
    "def filter_kaggle_dataframe_by_tickers(\n",
    "    df: pd.DataFrame,\n",
    "    tickers: list,\n",
    "    date_col: str = \"Date\",\n",
    "    start_date: str = \"2020-01-01\",\n",
    "    end_date: str = \"2023-12-31\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter a Kaggle-based DataFrame for the given tickers and a date range, returning a new DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame containing stock records (with a 'Ticker' column).\n",
    "    tickers : list\n",
    "        List of tickers to keep.\n",
    "    date_col : str\n",
    "        Name of the column holding dates. Default \"Date\".\n",
    "    start_date : str\n",
    "        Earliest date in \"YYYY-MM-DD\" format. Default \"2020-01-01\".\n",
    "    end_date : str\n",
    "        Latest date in \"YYYY-MM-DD\" format. Default \"2023-12-31\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Filtered DataFrame with rows only for the specified tickers and date range.\n",
    "        'Date' column is converted to datetime.\n",
    "    \"\"\"\n",
    "    # Keep only rows that match top tickers\n",
    "    df_filtered = df[df['Ticker'].isin(tickers)].copy()\n",
    "\n",
    "    # Ensure date_col is datetime, then filter by date range\n",
    "    df_filtered[date_col] = pd.to_datetime(df_filtered[date_col], errors='coerce')\n",
    "    mask = (df_filtered[date_col] >= start_date) & (df_filtered[date_col] <= end_date)\n",
    "    df_filtered = df_filtered[mask]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def resample_to_quarterly(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = \"Date\",\n",
    "    agg_dict: dict = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resample a DataFrame (with Ticker & a date index) to quarterly frequency using the specified aggregations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Stock DataFrame with columns including 'Ticker', 'Open','High','Low','Close','Volume','Dividends','Stock Splits'.\n",
    "    date_col : str\n",
    "        The column that will become the index for resampling. Default \"Date\".\n",
    "    agg_dict : dict\n",
    "        A dict mapping column -> aggregation function(s). If None is provided,\n",
    "        a default is used for typical OHLC resampling.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Resampled DataFrame, grouped by Ticker, then aggregated quarterly, then reset to a normal index.\n",
    "    \"\"\"\n",
    "    if agg_dict is None:\n",
    "        agg_dict = {\n",
    "            'Open': 'first',\n",
    "            'High': 'max',\n",
    "            'Low': 'min',\n",
    "            'Close': 'last',\n",
    "            'Volume': 'sum',\n",
    "            'Dividends': 'sum',\n",
    "            'Stock Splits': 'sum'\n",
    "        }\n",
    "\n",
    "    # Set the date_col as index, group by Ticker, then resample\n",
    "    df_copy = df.copy()\n",
    "    df_copy.set_index(date_col, inplace=True)\n",
    "    # groupby Ticker, resample quarterly\n",
    "    quarterly_df = (df_copy.groupby('Ticker')\n",
    "                            .resample('Q')\n",
    "                            .agg(agg_dict)\n",
    "                            .reset_index())\n",
    "    return quarterly_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Extract top 10 tickers from SSGA Excel files\n",
    "    folder_path = \"/home/pinagm/dev/Udacity_DataAnalyst_NanoDegree/D502 - Capstone Project/SSGA Data\"\n",
    "    top_tickers = extract_top_tickers_from_ssga(folder_path, n=10)\n",
    "    print(f\"Top tickers from SSGA: {top_tickers}\")\n",
    "\n",
    "    # 2) Filter an existing Kaggle-based DataFrame with top_tickers & date range\n",
    "    filtered_kag_df = pd.read_csv(\"Kaggle Data/all_stock_data.csv\")\n",
    "\n",
    "    df_top = filter_kaggle_dataframe_by_tickers(\n",
    "        filtered_kag_df,\n",
    "        top_tickers,\n",
    "        date_col=\"Date\",\n",
    "        start_date=\"2010-01-01\",\n",
    "        end_date=\"2024-12-31\"\n",
    "    )\n",
    "\n",
    "    # 3) Resample that filtered DataFrame to quarterly frequency\n",
    "    quarterly_kag_df = resample_to_quarterly(df_top)\n",
    "\n",
    "    # 4) Preview & save the quarterly DataFrame\n",
    "    print(quarterly_kag_df.head())\n",
    "    output_path = \"/home/pinagm/dev/Udacity_DataAnalyst_NanoDegree/D502 - Capstone Project/Kaggle Data/filtered_stock_data_quarterly.csv\"\n",
    "    quarterly_kag_df.to_csv(output_path, index=False)\n",
    "    print(f\"Quarterly data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_if_exists(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    If the specified Excel file exists, read it into a DataFrame and print its head.\n",
    "    Otherwise, print a 'File not found' message and return None.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to the Excel file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        The loaded DataFrame, or None if the file does not exist.\n",
    "    \"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        df = pd.read_excel(filepath)\n",
    "        print(f\"Loaded Excel file: {filepath}\")\n",
    "        print(df.head())\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return None\n",
    "\n",
    "def load_csv_if_exists(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    If the specified CSV file exists, read it into a DataFrame and print its head.\n",
    "    Otherwise, print a 'File not found' message and return None.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to the CSV file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        The loaded DataFrame, or None if the file does not exist.\n",
    "    \"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Loaded CSV file: {filepath}\")\n",
    "        print(df.head())\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    current_date = datetime.now().strftime('%m-%d-%Y')\n",
    "    bea_excel = f'BEA Data/bea-gdp-by-industry-filtered-{current_date}.xlsx'\n",
    "    kag_csv = 'Kaggle Data/filtered_stock_data_quarterly.csv'\n",
    "    \n",
    "    bea_excel_df = load_excel_if_exists(bea_excel)\n",
    "    kag_csv_df = load_csv_if_exists(kag_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_map = {'I': '1', 'II': '2', 'III': '3', 'IV': '4'}\n",
    "\n",
    "def parse_quarter_str(qstr: str) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    Convert strings like '2020QI' or '2021QIII' into the end-of-quarter date:\n",
    "    e.g. '2020-03-31'.\n",
    "\n",
    "    Returns a pd.Timestamp, or None if format is invalid.\n",
    "    \"\"\"\n",
    "    match = re.match(r\"^(\\d{4})Q(I|II|III|IV)$\", qstr)\n",
    "    if match:\n",
    "        year_str = match.group(1)           # e.g. '2020'\n",
    "        roman_quarter = match.group(2)      # e.g. 'II'\n",
    "        year = int(year_str)\n",
    "        quarter_num = int(roman_map[roman_quarter])  # 1..4\n",
    "        period = pd.Period(year=year, quarter=quarter_num, freq='Q')\n",
    "        return period.end_time\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def melt_bea_quarters(\n",
    "    df: pd.DataFrame,\n",
    "    id_cols: list = [\"Industry\",\"IndustryDescription\",\"Sector\"]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Melt a wide BEA DataFrame with quarter columns (e.g., '2020QI','2020QII',...)\n",
    "    into a long format, creating a 'QuarterEnd' column for each quarter.\n",
    "    \"\"\"\n",
    "    # Identify quarter columns that contain 'Q' but aren't in id_cols\n",
    "    quarter_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in id_cols\n",
    "        and 'Q' in c\n",
    "    ]\n",
    "\n",
    "    # Melt from wide to long\n",
    "    melted = df.melt(\n",
    "        id_vars=id_cols,\n",
    "        value_vars=quarter_cols,\n",
    "        var_name='QuarterStr',\n",
    "        value_name='GDP'\n",
    "    )\n",
    "\n",
    "    # Convert QuarterStr to a real end-of-quarter date\n",
    "    melted['QuarterEnd'] = melted['QuarterStr'].apply(parse_quarter_str)\n",
    "\n",
    "    # Drop rows with invalid QuarterStr\n",
    "    melted.dropna(subset=['QuarterEnd'], inplace=True)\n",
    "\n",
    "    # Convert QuarterEnd to last day of that quarter\n",
    "    melted['QuarterEnd'] = (\n",
    "        melted['QuarterEnd'].dt.to_period('Q')\n",
    "                             .dt.to_timestamp(freq='Q')\n",
    "    )\n",
    "\n",
    "    # Drop QuarterStr\n",
    "    melted.drop(columns=['QuarterStr'], inplace=True)\n",
    "\n",
    "    return melted\n",
    "\n",
    "def convert_date_to_quarter_end(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = \"Date\",\n",
    "    out_col: str = \"QuarterEnd\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a date column to the last day of its quarter in a new column (out_col).\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy[date_col] = pd.to_datetime(df_copy[date_col], errors='coerce')\n",
    "    df_copy[out_col] = (\n",
    "        df_copy[date_col].dt.to_period('Q')\n",
    "                           .dt.to_timestamp(freq='Q')\n",
    "    )\n",
    "    return df_copy\n",
    "\n",
    "def load_filtered_bea_file(folder: str = \"BEA Data\", keyword: str = \"filtered\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Search 'folder' for an .xlsx file containing 'keyword' in its filename.\n",
    "    Return a DataFrame loaded from the first match, or None if none found.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(folder, f\"*{keyword}*.xlsx\")\n",
    "    matching_files = glob.glob(pattern)\n",
    "    if not matching_files:\n",
    "        print(f\"No .xlsx file with '{keyword}' found in '{folder}'.\")\n",
    "        return None\n",
    "\n",
    "    first_match = matching_files[0]\n",
    "    print(f\"Reading '{first_match}'...\")\n",
    "    return pd.read_excel(first_match)\n",
    "\n",
    "def load_csv_if_exists(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    If the CSV file at 'filepath' exists, read it into a DataFrame and show its head.\n",
    "    Otherwise, return None.\n",
    "    \"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Loaded CSV file: {filepath}\")\n",
    "        print(df.head())\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 1) Load the \"filtered\" BEA Excel file automatically\n",
    "    bea_excel_df = load_filtered_bea_file(\"BEA Data\", keyword=\"filtered\")\n",
    "    if bea_excel_df is None:\n",
    "        print(\"Cannot proceed: No filtered BEA file found.\")\n",
    "    else:\n",
    "        print(\"Loaded BEA Excel data (head):\")\n",
    "        print(bea_excel_df.head())\n",
    "\n",
    "        # 2) Melt the wide BEA DataFrame\n",
    "        bea_melted = melt_bea_quarters(bea_excel_df)\n",
    "        print(\"\\nMelted BEA Data (head):\")\n",
    "        print(bea_melted.head(10))\n",
    "\n",
    "    # 3) Load the Kaggle CSV if it exists\n",
    "    kag_csv_path = \"Kaggle Data/filtered_stock_data_quarterly.csv\"\n",
    "    kag_csv_df = load_csv_if_exists(kag_csv_path)\n",
    "    if kag_csv_df is None:\n",
    "        print(\"Cannot proceed: Kaggle CSV not found.\")\n",
    "    else:\n",
    "        print(\"\\nKaggle CSV initial (head):\")\n",
    "        print(kag_csv_df.head())\n",
    "\n",
    "        # 4) Convert Kaggle date col to quarter-end\n",
    "        kag_csv_df = convert_date_to_quarter_end(kag_csv_df, date_col=\"Date\", out_col=\"QuarterEnd\")\n",
    "        print(\"\\nKaggle CSV after quarter-end conversion (head):\")\n",
    "        print(kag_csv_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_and_combine_ssga_data(\n",
    "    ssga_df_list: List[pd.DataFrame],\n",
    "    etf_list: List[str],\n",
    "    etf_to_sector: Dict[str, str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Annotate each SSGA DataFrame with the corresponding ETF ticker and sector,\n",
    "    then concatenate them all into one combined DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ssga_df_list : List[pd.DataFrame]\n",
    "        A list of DataFrames, one per ETF, typically from fetch_and_process_etf_data().\n",
    "    etf_list : List[str]\n",
    "        The list of ETF tickers in the same order as ssga_df_list (e.g., ['XLI','XLK','XLE','XLB']).\n",
    "    etf_to_sector : Dict[str, str]\n",
    "        A mapping from ETF symbol to sector name (e.g., {'XLI':'Industrials','XLK':'Technology'}).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A single DataFrame containing all rows from ssga_df_list, with added columns:\n",
    "        - HoldingTicker (renamed from original 'Ticker')\n",
    "        - ETF_Ticker\n",
    "        - Sector\n",
    "    \"\"\"\n",
    "    updated_ssga_dfs = []\n",
    "\n",
    "    # Loop over each ETF name + SSGA DataFrame\n",
    "    for etf_name, df in zip(etf_list, ssga_df_list):\n",
    "        # Rename 'Ticker' -> 'HoldingTicker' to avoid confusion\n",
    "        df = df.rename(columns={'Ticker': 'HoldingTicker'}).copy()\n",
    "\n",
    "        # Add a column for the ETF ticker\n",
    "        df['ETF_Ticker'] = etf_name\n",
    "\n",
    "        # Use the mapping to find the sector\n",
    "        if etf_name in etf_to_sector:\n",
    "            df['Sector'] = etf_to_sector[etf_name]\n",
    "        else:\n",
    "            df['Sector'] = 'Unknown'  # or raise an error/warning\n",
    "\n",
    "        updated_ssga_dfs.append(df)\n",
    "\n",
    "    # Concatenate all the updated DataFrames\n",
    "    combined_ssga_df = pd.concat(updated_ssga_dfs, ignore_index=True)\n",
    "    return combined_ssga_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etf_list = ['XLI', 'XLK', 'XLE', 'XLB']\n",
    "    etf_to_sector_map = {\n",
    "        'XLI': 'Industrials',\n",
    "        'XLK': 'Technology',\n",
    "        'XLE': 'Energy',\n",
    "        'XLB': 'Materials'\n",
    "    }\n",
    "    \n",
    "    combined_ssga_df = annotate_and_combine_ssga_data(\n",
    "        ssga_df_list,\n",
    "        etf_list,\n",
    "        etf_to_sector_map\n",
    "    )\n",
    "\n",
    "    print(\"Combined SSGA DataFrame (head):\")\n",
    "    print(combined_ssga_df.head(10))\n",
    "    print(\"ETF_Ticker values:\", combined_ssga_df['ETF_Ticker'].unique())\n",
    "    print(\"Sector values:\", combined_ssga_df['Sector'].unique())\n",
    "    \n",
    "xli_holdings = combined_ssga_df[combined_ssga_df['ETF_Ticker'] == 'XLI']\n",
    "print(xli_holdings.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Kaggle DF with combined SSGA DF\n",
    "merged_kag_ssga = pd.merge(\n",
    "    kag_csv_df,  # from Kaggle\n",
    "    combined_ssga_df, \n",
    "    left_on='Ticker',     # from Kaggle\n",
    "    right_on='HoldingTicker',  # from SSGA\n",
    "    how='left'           # keep all Kaggle rows, even if no match\n",
    ")\n",
    "\n",
    "print(\"After merging Kaggle + SSGA, shape:\", merged_kag_ssga.shape)\n",
    "print(merged_kag_ssga.head())\n",
    "bea_sector_agg = (\n",
    "    bea_melted\n",
    "    .groupby([\"Sector\",\"QuarterEnd\"], as_index=False)\n",
    "    .agg({\"GDP\": \"sum\"}) \n",
    ")\n",
    "\n",
    "final_merged_df = pd.merge(\n",
    "    merged_kag_ssga,\n",
    "    bea_sector_agg,\n",
    "    on=[\"Sector\",\"QuarterEnd\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"Final shape:\", final_merged_df.shape)\n",
    "print(final_merged_df.tail())\n",
    "\n",
    "# Group by Sector and describe\n",
    "grouped_stats = final_merged_df.groupby('Sector')[['Close','Volume','GDP']].describe()\n",
    "print(grouped_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=final_merged_df, x='Sector', y='Close')\n",
    "plt.title(\"Distribution of Closing Prices by Sector\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ETF and Ticker, and sum the weights\n",
    "grouped_df = final_merged_df.groupby(['ETF', 'Ticker']).agg({'Weight': 'sum'}).reset_index()\n",
    "\n",
    "# Sort tickers within each ETF by weight in descending order and get the top 5\n",
    "top_tickers_df = grouped_df.sort_values(by=['ETF', 'Weight'], ascending=[True, False]).groupby('ETF').head(5)\n",
    "\n",
    "# Plotting the top 5 tickers for each ETF\n",
    "etfs = top_tickers_df['ETF'].unique()\n",
    "\n",
    "for etf in etfs:\n",
    "    top_tickers = top_tickers_df[top_tickers_df['ETF'] == etf]['Ticker']\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for ticker in top_tickers:\n",
    "        df_ticker = final_merged_df[final_merged_df['Ticker'] == ticker].copy()\n",
    "        plt.plot(df_ticker['QuarterEnd'], df_ticker['Close'], marker='o', label=ticker)\n",
    "    \n",
    "    plt.title(f\"Top 5 Tickers by Weight for {etf}\")\n",
    "    plt.xlabel(\"QuarterEnd\")\n",
    "    plt.ylabel(\"Close Price\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_qtr = (final_merged_df\n",
    "    .groupby(['Sector','QuarterEnd'], as_index=False)\n",
    "    .agg({'Close':'mean','GDP':'mean'})  # mean GDP is the same as the single GDP if aggregated, but you can do it so yolo\n",
    ")\n",
    "\n",
    "# Then plot a line chart\n",
    "for sector_name in sector_qtr['Sector'].unique():\n",
    "    sub = sector_qtr[sector_qtr['Sector'] == sector_name]\n",
    "    plt.plot(sub['QuarterEnd'], sub['Close'], label=f\"{sector_name} Avg Close\")\n",
    "\n",
    "plt.title(\"Average Quarterly Close by Sector\")\n",
    "plt.xlabel(\"QuarterEnd\")\n",
    "plt.ylabel(\"Average Close\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_qtr = (final_merged_df\n",
    "    .groupby(['Sector','QuarterEnd'], as_index=False)\n",
    "    .agg({'GDP':'mean'})  # mean GDP is the same as the single GDP if aggregated, but you can do it so yolo\n",
    ")\n",
    "\n",
    "# Then plot a line chart\n",
    "for sector_name in sector_qtr['Sector'].unique():\n",
    "    sub = sector_qtr[sector_qtr['Sector'] == sector_name]\n",
    "    plt.plot(sub['QuarterEnd'], sub['GDP'], label=f\"{sector_name} Avg GDP\")\n",
    "\n",
    "plt.title(\"Average Quarterly GDP by Sector\")\n",
    "plt.xlabel(\"QuarterEnd\")\n",
    "plt.ylabel(\"Average GDP\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Group final_merged_df by Sector, QuarterEnd, and compute mean GDP & mean Close\n",
    "df_agg = (final_merged_df\n",
    "          .groupby(['Sector','QuarterEnd'], as_index=False)\n",
    "          .agg({\n",
    "              'GDP': 'mean',    \n",
    "              'Close': 'mean'   \n",
    "          }))\n",
    "\n",
    "print(df_agg.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = df_agg['Sector'].unique()\n",
    "fig, axes = plt.subplots(nrows=len(sectors), ncols=1, figsize=(8, 4*len(sectors)), sharex=True)\n",
    "\n",
    "if len(sectors) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, sector_name in zip(axes, sectors):\n",
    "    sub = df_agg[df_agg['Sector'] == sector_name].sort_values('QuarterEnd')\n",
    "\n",
    "    # Left axis: GDP\n",
    "    ax.plot(sub['QuarterEnd'], sub['GDP'], color='blue', marker='o', label='GDP')\n",
    "    ax.set_ylabel(\"GDP\", color='blue')\n",
    "    ax.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Create a second axis sharing the same x\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(sub['QuarterEnd'], sub['Close'], color='red', marker='o', label='Avg Price')\n",
    "    ax2.set_ylabel(\"Stock Price\", color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "    ax.set_title(f\"{sector_name} - GDP vs. Stock Price (Dual Axis)\")\n",
    "    ax.set_xlabel(\"QuarterEnd\")\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cols = ['Close','Volume','Dividends','Weight','GDP']\n",
    "corr_matrix = final_merged_df[corr_cols].corr()\n",
    "print(corr_matrix)\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='viridis')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sector_name in final_merged_df['Sector'].unique():\n",
    "    sub = final_merged_df[final_merged_df['Sector'] == sector_name]\n",
    "    corr_val = sub['Close'].corr(sub['GDP'])\n",
    "    print(f\"Correlation between Close and GDP in {sector_name}: {corr_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Sort our DataFrame so groupby shifts occur in the correct chronological order\n",
    "final_merged_df = final_merged_df.sort_values(by=['Sector', 'QuarterEnd'])\n",
    "\n",
    "# Ensure 'Pct_Return' column is created\n",
    "final_merged_df['Pct_Return'] = final_merged_df.groupby('Ticker')['Close'].pct_change()\n",
    "\n",
    "# 2) Compute quarter-over-quarter GDP growth per sector\n",
    "#    pct_change() calculates (current - previous) / previous\n",
    "final_merged_df['GDP_growth'] = final_merged_df.groupby('Sector')['GDP'].pct_change()\n",
    "\n",
    "# 3) Create a lag of 1 quarter for GDP growth\n",
    "final_merged_df['GDP_growth_lag1'] = final_merged_df.groupby('Sector')['GDP_growth'].shift(1)\n",
    "\n",
    "# 4) For each sector, run OLS with Pct_Return as the dependent variable\n",
    "#    and last quarter's GDP growth as the predictor\n",
    "sectors = final_merged_df['Sector'].unique()\n",
    "for sector_name in sectors:\n",
    "    # Filter to that sector, drop rows with NaN in either Pct_Return or GDP_growth_lag1\n",
    "    sub_df = final_merged_df[(final_merged_df['Sector'] == sector_name) & \n",
    "                      (~final_merged_df['Pct_Return'].isna()) & \n",
    "                      (~final_merged_df['GDP_growth_lag1'].isna())]\n",
    "    \n",
    "    # Check we have enough data\n",
    "    if len(sub_df) > 2:\n",
    "        model = smf.ols(\"Pct_Return ~ GDP_growth_lag1\", data=sub_df).fit()\n",
    "        print(f\"\\n=== Sector: {sector_name} ===\")\n",
    "        print(model.summary())\n",
    "    else:\n",
    "        print(f\"\\n=== Sector: {sector_name} ===\")\n",
    "        print(\"Not enough data points after dropping NaNs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df = final_merged_df.sort_values(by=['Ticker','QuarterEnd'])\n",
    "final_merged_df['Pct_Return'] = final_merged_df.groupby('Ticker')['Close'].pct_change()\n",
    "final_merged_df['GDP_scaled'] = final_merged_df['GDP'] / 1000_000  # e.g., in billions\n",
    "model_df = final_merged_df.dropna(subset=['Pct_Return','GDP_growth_lag1'])\n",
    "result = smf.ols(\"Pct_Return ~ GDP_growth_lag1\", data=model_df).fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the regression for each sector with enough data\n",
    "for sector_name in model_df['Sector'].unique():\n",
    "    sub = model_df[model_df['Sector'] == sector_name]\n",
    "    if len(sub) > 10:  # need enough data points\n",
    "        reg_res = smf.ols(\"Pct_Return ~ GDP_growth_lag1\", data=sub).fit()\n",
    "        print(f\"\\n=== Sector: {sector_name} ===\")\n",
    "        print(reg_res.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for the focus sectors\n",
    "sns.lmplot(data=final_merged_df, x='GDP', y='Close', hue='Sector', ci=None)\n",
    "plt.title(\"Close Price vs. GDP by Sector\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to the 'Materials' sector and remove extreme Pct_Return values\n",
    "df_mat = final_merged_df[final_merged_df['Sector'] == 'Materials'].dropna(subset=['Pct_Return','GDP_growth_lag1'])\n",
    "df_mat = df_mat[(df_mat['Pct_Return'] > -0.3) & (df_mat['Pct_Return'] < 0.3)]\n",
    "df_mat['GDP_growth_lag1_pct'] = df_mat['GDP_growth_lag1'] * 100\n",
    "sns.regplot(data=df_mat, x='GDP_growth_lag1_pct', y='Pct_Return')\n",
    "plt.title(\"Materials: Next-Quarter Return vs. Previous Quarter GDP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale GDP from e.g. millions to billions\n",
    "final_merged_df['GDP_bln'] = final_merged_df['GDP'] / 1e3\n",
    "\n",
    "# Re-run the regression but replace GDP_lag1 with scaled version\n",
    "final_merged_df['GDP_bln_lag1'] = final_merged_df.groupby('Sector')['GDP_bln'].shift(1)\n",
    "\n",
    "model_mat_bln = smf.ols(\"Pct_Return ~ GDP_bln_lag1\", data=final_merged_df[final_merged_df['Sector'] == 'Materials']).fit()\n",
    "print(model_mat_bln.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by Sector and QuarterEnd\n",
    "final_merged_df = final_merged_df.sort_values(by=['Sector','QuarterEnd'])\n",
    "final_merged_df['GDP_growth'] = final_merged_df.groupby('Sector')['GDP'].pct_change()\n",
    "final_merged_df['GDP_growth_lag1'] = final_merged_df.groupby('Sector')['GDP_growth'].shift(1)\n",
    "\n",
    "model_mat_growth = smf.ols(\"Pct_Return ~ GDP_growth_lag1\", data=final_merged_df[final_merged_df['Sector'] == 'Materials']).fit()\n",
    "print(model_mat_growth.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Extract the features used in our model\n",
    "X = df_mat[['GDP_growth_lag1']] # independent variable\n",
    "\n",
    "# 2) True target values\n",
    "y = df_mat['Pct_Return'] # dependent variable\n",
    "\n",
    "# 3) Predicted values from the model\n",
    "y_pred = result.predict(X) # y_pred = b0 + b1*X\n",
    "\n",
    "# 4) Plot predicted vs. actual\n",
    "plt.scatter(y_pred, y, alpha=0.7)\n",
    "plt.xlabel(\"Predicted (Pct_Return)\")\n",
    "plt.ylabel(\"Actual (Pct_Return)\")\n",
    "plt.title(\"Predicted vs. Actual Quarterly Returns (OLS)\")\n",
    "\n",
    "# Adding a diagonal line of perfect prediction\n",
    "lims = [min(y.min(), y_pred.min()), max(y.max(), y_pred.max())]\n",
    "plt.plot(lims, lims, 'r--')  # dashed red diagonal\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y - y_pred\n",
    "\n",
    "plt.scatter(y_pred, residuals, alpha=0.7)\n",
    "plt.axhline(0, color='red', linestyle='--')  # zero-residual line\n",
    "plt.xlabel(\"Predicted (Pct_Return)\")\n",
    "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
    "plt.title(\"Residual Plot (OLS)\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "author": "Gregory M Pina",
  "kernelspec": {
   "display_name": "WGU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "title": "Investigating the Lagged Impact of Sectoral GDP Growth on Stock Price Performance"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
