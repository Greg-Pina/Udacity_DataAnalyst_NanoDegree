{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze A/B Test Results\n",
    "\n",
    "This project will assure you have mastered the subjects covered in the statistics lessons.  The hope is to have this project be as comprehensive of these topics as possible.  Good luck!\n",
    "\n",
    "Corresponding with this notebook is a slide deck where you will need to update all the portions in red.  Completing the notebook will provide all the results needed for the slides.  **Correctly completing the slides is a required part of the project.**\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Descriptive Statistics](#descriptive)\n",
    "- [Part II - Probability](#probability)\n",
    "- [Part III - Experimentation](#experimentation)\n",
    "- [Part IV - Algorithms](#algorithms)\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "A/B tests are very commonly performed by data analysts and data scientists.  For this project, you will be working to understand the results of an A/B test run by an e-commerce website.  Your goal is to work through this notebook to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision.\n",
    "\n",
    "**As you work through this notebook, follow along in the classroom and answer the corresponding quiz questions associated with each question.** The labels for each classroom concept are provided for each question.  This will assure you are on the right track as you work through the project, and you can feel more confident in your final submission meeting the criteria.  As a final check, assure you meet all the criteria on the [RUBRIC](https://review.udacity.com/#!/projects/37e27304-ad47-4eb0-a1ab-8c12f60e43d0/rubric).\n",
    "\n",
    "<a id='descriptive'></a>\n",
    "#### Part I - Descriptive Statistics\n",
    "\n",
    "To get started, let's import our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#We are setting the seed to assure you get the same answers on quizzes as we set up\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the parts of question `1` notice links to [pandas documentation](https://pandas.pydata.org/) is provided to assist with answering the questions.  Though there are other ways you could solve the questions, the documentation is provided to assist you with one fast way to find the answer to each question.\n",
    "\n",
    "\n",
    "`1.a)` Now, read in the `ab_data.csv` data. Store it in `df`. Read in the dataset and take a look at the top few rows here. **This question is completed for you**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ab_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mab_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ab_data.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('ab_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`b)` Use the below cell to find the number of rows in the dataset. [Helpful  Pandas Link](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html#pandas.DataFrame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the number of rows in the dataset\n",
    "num_rows = df.shape[0]\n",
    "num_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`c)` The proportion of users converted.  [Helpful  Pandas Link](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mean.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the proportion of users who converted\n",
    "conversion_proportion = df['converted'].mean()\n",
    "\n",
    "# Convert to percentage and format with %\n",
    "conversion_percentage = f\"{conversion_proportion * 100:.2f}%\"\n",
    "\n",
    "conversion_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d)` Do any of the rows have missing values? [Helpful Pandas Link One](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isnull.html) and [Helpful Pandas Link Two](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values_present = missing_values.any()\n",
    "missing_values_present, missing_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`e)` How many customers are from each country? Build a bar chart to show the count of visits from each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of visitors from each country - pull the necessary code from the next cell to provide just the counts\n",
    "df['country'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar chart of results - this part is done for you\n",
    "df['country'].value_counts().plot(kind='bar');\n",
    "plt.title('Number of Visits From Each Country');\n",
    "plt.ylabel('Count of Visits');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f)` Recognize that all of your columns are of a **categorical data type** with the exception of one.  Which column is not **categorical**? [Helpful Pandas Link](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data types of all columns\n",
    "data_info = df.info()\n",
    "\n",
    "# Check which column is not categorical\n",
    "non_categorical_columns = df.select_dtypes(exclude=['object', 'category']).columns.tolist()\n",
    "non_categorical_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`g)` What are the possible values of the `converted` column?  Does it make sense that these values are the only possible values? Why or why not? \n",
    "\n",
    "**Here you can use one of the functions you used in an earlier question**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the unique values in the 'converted' column\n",
    "converted_unique_values = df['converted'].unique()\n",
    "converted_unique_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does it make sense that these are the only values?**\n",
    "\n",
    "Yes, it makes sense. The converted column represents a binary outcome:\n",
    "\n",
    "- 0: The user did not convert.\n",
    "- 1: The user converted.\n",
    "\n",
    "Since the column is intended to record whether a user converted or not, having only these two values aligns with the expected representation of binary data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='probability'></a>\n",
    "#### Part II - Probability\n",
    "\n",
    "`1.` Now that you have had a chance to learn more about the dataset, let's look more at how different factors are related to `converting`.\n",
    "\n",
    "`a)` What is the probability of an individual converting regardless of the page they receive or the country they are from? Simply, what is the chance of conversion in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the overall probability of conversion\n",
    "overall_conversion_probability = df['converted'].mean()\n",
    "overall_conversion_probability = f\"{overall_conversion_probability * 100:.2f}%\"\n",
    "overall_conversion_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of an individual converting, regardless of the page they receive or the country they are from, is approximately **13.05%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`b)` Given that an individual was in the `control` group, what is the probability they converted? **This question is completed for you**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.query('group == \"control\"')['converted'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`c)` Given that an individual was in the `treatment` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of conversion for individuals in the treatment group\n",
    "treatment_conversion_probability = df[df['group'] == 'treatment']['converted'].mean()\n",
    "treatment_conversion_probability = f\"{treatment_conversion_probability * 100:.2f}%\"\n",
    "treatment_conversion_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d)` Do you see evidence that the treatment is related to higher `converted` rates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate whether the treatment is related to higher conversion rates, we can consider the following:\n",
    "\n",
    "<ol>\n",
    "<li><b>Probability Comparison:</b></li>\n",
    "<ul>\n",
    "<li>Conversion probability for the control group: <b>10.53%</b></li>\n",
    "<li>Conversion probability for the treatment group: <b>15.53%</b></li>\n",
    "</ul>\n",
    "<p>The treatment group has a higher conversion rate compared to the control group, suggesting a potential relationship between being in the treatment group and higher conversion rates.</p>\n",
    "<li><b>Statistical Significance:</b></li>\n",
    "<ul>\n",
    "<li>While the probabilities suggest a higher conversion rate for the treatment group, we cannot definitively conclude a causal relationship without conducting a statistical test (e.g., a two-proportion z-test). This would help determine whether the observed difference is statistically significant or due to random chance.</li>\n",
    "</ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`e)` What is the probability that an individual was in the `treatment`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the probability of an individual being in the treatment group\n",
    "treatment_probability = (df['group'] == 'treatment').mean()\n",
    "treatment_probability = f\"{treatment_probability * 100:.2f}%\"\n",
    "treatment_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f)` What is the probability that an individual was from Canada `CA`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of an individual being from Canada (CA)\n",
    "canada_probability = (df['country'] == 'CA').mean()\n",
    "canada_probability = f\"{canada_probability * 100:.2f}%\"\n",
    "canada_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`g)` Given that an individual was in the `US`, what was the probability that they `converted`? **This question is completed for you**\n",
    "\n",
    "$P(\\text{converted} == 1|\\text{country} ==\\text{\"US\"})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('country == \"US\"')['converted'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`h)` Given that an individual was in the `UK`, what was the probability that they `converted`? \n",
    "\n",
    "$P(\\text{converted} == 1|\\text{country} ==\\text{\"UK\"})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_probability = df.query('country == \"UK\"')['converted'].mean()\n",
    "uk_probability = f\"{uk_probability * 100:.2f}%\"\n",
    "uk_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`i)` Do you see evidence that the `converted` rate might differ from one country to the next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To evaluate whether the conversion rate differs between countries, we can examine the conversion rates for each country\n",
    "and perform statistical tests if needed.\"\"\"\n",
    "\n",
    "#Let’s calculate the conversion rates for each country first.\n",
    "# Calculate conversion rates by country\n",
    "conversion_rates_by_country = df.groupby('country')['converted'].mean()\n",
    "conversion_rates_by_country\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed conversion rates by country are as follows:\n",
    "\n",
    "- **Canada (CA)**: 12.53%\n",
    "- **United Kingdom (UK)**: 12.51%\n",
    "- **United States (US)**: 13.28%\n",
    "\n",
    "#### **Evidence of Differences**\n",
    "\n",
    "There is a slight variation in conversion rates across countries. However, to conclude whether these differences are statistically significant, a hypothesis test (e.g., chi-squared test or a proportion z-test) would be required.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`j)` Consider the table below, fill in the conversion rates below to look at how conversion by country and treatment group vary.  The `US` column is done for you, and two methods for calculating the probabilities are shown - **COMPLETE THE REST OF THE TABLE**.  Does it appear that there could be an interaction between how country and treatment impact conversion?\n",
    "\n",
    "These two values that are filled in can be written as:\n",
    "\n",
    "$P(\\text{converted} == 1|(\\text{country} ==\\text{\"US\" AND }\\text{group} ==\\text{\"control\"})) = 10.7\\%$\n",
    "\n",
    "$P(\\text{converted} == 1|(\\text{country} ==\\text{\"US\" AND }\\text{group} ==\\text{\"treatment\"})) = 15.8\\%$\n",
    "\n",
    "|             | US          | UK          | CA          |\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| Control     | 10.7%       |  %          |  %          |\n",
    "| Treatment   | 15.8%       |  %          |  %          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1  - explicitly calculate each probability\n",
    "print(df.query('country == \"US\" and group == \"control\" and converted == 1').shape[0]/df.query('country == \"US\" and group == \"control\"').shape[0]) \n",
    "print(df.query('country == \"US\" and group == \"treatment\" and converted == 1').shape[0]/df.query('country == \"US\" and group == \"treatment\"').shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2 - quickly calculate using `groupby`\n",
    "df.query('country == \"US\"').groupby('group')['converted'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution -- Complete the Table Here\n",
    "\n",
    "|             | US          | UK          | CA          |\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| Control     | 10.7%       |  10.16%          |  9.45%          |\n",
    "| Treatment   | 15.8%       |  14.87%          |  15.40%          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Interpretation**\n",
    "The table shows variations in conversion rates between the control and treatment groups within each country. Specifically:\n",
    "\n",
    "- The **treatment group** consistently has higher conversion rates than the control group across all countries.\n",
    "- The magnitude of the increase in conversion rates varies by country, which may suggest an interaction effect between the country and the treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate conversion rates by country and group using Method 2 (groupby)\n",
    "conversion_rates_table = df.groupby(['country', 'group'])['converted'].mean() * 100\n",
    "\n",
    "# Extract values for the table\n",
    "control_us = conversion_rates_table.loc[('US', 'control')]\n",
    "treatment_us = conversion_rates_table.loc[('US', 'treatment')]\n",
    "control_uk = conversion_rates_table.loc[('UK', 'control')]\n",
    "treatment_uk = conversion_rates_table.loc[('UK', 'treatment')]\n",
    "control_ca = conversion_rates_table.loc[('CA', 'control')]\n",
    "treatment_ca = conversion_rates_table.loc[('CA', 'treatment')]\n",
    "\n",
    "control_us, treatment_us, control_uk, treatment_uk, control_ca, treatment_ca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='experimentation'></a>\n",
    "### Part III - Experimentation\n",
    "\n",
    "`1.` Consider you need to make the decision just based on all the data provided.  If you want to assume that the control page is better unless the treatment page proves to be definitely better at a Type I error rate of 5%, you state your null and alternative hypotheses in terms of **$p_{control}$** and **$p_{treatment}$** as:  \n",
    "\n",
    "$H_{0}: p_{control} >= p_{treatment}$\n",
    "\n",
    "$H_{1}: p_{control} < p_{treatment}$\n",
    "\n",
    "Which is equivalent to:\n",
    "\n",
    "$H_{0}: p_{treatment} - p_{control} <= 0$\n",
    "\n",
    "$H_{1}: p_{treatment} - p_{control} > 0$\n",
    "\n",
    "\n",
    "Where  \n",
    "* **$p_{control}$** is the `converted` rate for the control page\n",
    "* **$p_{treatment}$** `converted` rate for the treatment page\n",
    "\n",
    "**Note for this experiment we are not looking at differences associated with country.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume under the null hypothesis, $p_{treatment}$ and $p_{control}$ both have \"true\" success rates equal to the **converted** success rate regardless of page - that is $p_{treatment}$ and $p_{control}$ are equal. Furthermore, assume they are equal to the **converted** rate in `df` regardless of the page. **These are set in the first cell below.**<br><br>\n",
    "\n",
    "* Use a sample size for each page equal to the ones in `df`. **These are also set below.**  <br><br>\n",
    "\n",
    "* Perform the sampling distribution for the difference in `converted` between the two pages over 500 iterations of calculating an estimate from the null.  <br><br>\n",
    "\n",
    "* Use the cells below to provide the necessary parts of this simulation.  \n",
    "\n",
    "If this doesn't make complete sense right now, don't worry - you are going to work through the problems below to complete this problem.  You can use **Quiz 4** in the classroom to make sure you are on the right track.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`a)`The **convert rate** for $p_{control}$ under the null. The sample size for the `control` and the sample size for the `treatment` are from the original dataset. **All of these values are set below, and set the stage for the simulations you will run for the rest of this section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_control_treatment_null  = df['converted'].mean()\n",
    "n_treatment = df.query('group == \"treatment\"').shape[0]\n",
    "n_control = df.query('group == \"control\"').shape[0]\n",
    "p_control_treatment_null,n_treatment,n_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`b)` Use the results from part `a)` to simulate `n_treatment` transactions with a convert rate of `p_treatment_null`.  Store these $n_{treatment}$ 1's and 0's in a `list` of **treatment_converted**.  It should look something like the following (the 0's and and 1's **don't** need to be the same): \n",
    "\n",
    "`[0, 0, 1, 1, 0, ....]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# treatment_converted\n",
    "import numpy as np\n",
    "\n",
    "# Simulate n_treatment transactions with a conversion rate of p_treatment_null\n",
    "treatment_converted = np.random.binomial(1, p_control_treatment_null, n_treatment).tolist()\n",
    "\n",
    "# Display the first few values as a sample\n",
    "treatment_converted[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`c)` Use the results from part `a)` to simulate `n_control` transactions with a convert rate of `p_control_null`.  Store these $n_{treatment}$ 1's and 0's in a `list` of **control_converted**.  It should look something like the following (the 0's and and 1's **don't** need to be exactly the same): \n",
    "\n",
    "`[0, 0, 1, 1, 0, ....]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulate n_control transactions with a conversion rate of p_control_null\n",
    "control_converted = np.random.binomial(1, p_control_treatment_null, n_control).tolist()\n",
    "\n",
    "# Display the first few values as a sample\n",
    "control_converted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d)` Find the estimate for $p_{treatment}$ - $p_{control}$ under the null using the simulated values from part `(b)` and `(c)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the simulated conversion rates for treatment and control groups\n",
    "simulated_p_treatment = np.mean(control_converted)\n",
    "simulated_p_control = np.mean(control_converted)\n",
    "\n",
    "# Calculate the estimated difference under the null\n",
    "estimated_diff_null = simulated_p_treatment - simulated_p_control\n",
    "estimated_diff_null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated difference $p_{treatment}$ - $p_{control}$ under the null, based on the simulated values, is 0.0. This result aligns with the assumption under the null hypothesis that the two conversion rates are equal.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`e)` Simulate 500 $p_{treatment}$ - $p_{control}$ values using this same process as `b)`- `d)` similarly to the one you calculated in parts **a. through g.** above.  Store all 500 values in an numpy array called **p_diffs**.  This array should look similar to the below **(the values will not match AND this will likely take a bit of time to run)**:\n",
    "\n",
    "`[0.001, -0.003, 0.002, ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_diffs = []\n",
    "\n",
    "for _ in range(500):\n",
    "    # Simulate treatment and control conversions under the null\n",
    "    treatment_converted_sim = np.random.binomial(n_treatment, p_control_treatment_null) / n_treatment\n",
    "    control_converted_sim = np.random.binomial(n_control, p_control_treatment_null) / n_control\n",
    "    \n",
    "    # Calculate the difference in conversion rates for this simulation\n",
    "    diff = treatment_converted_sim - control_converted_sim\n",
    "    p_diffs.append(diff)\n",
    "\n",
    "# Convert to numpy array\n",
    "p_diffs = np.array(p_diffs)\n",
    "\n",
    "# Display the first few simulated differences\n",
    "p_diffs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f)` Plot a histogram of the **p_diffs**.  Does this plot look like what you expected?  Use the matching problem in the classroom to assure you fully understand what was computed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_diffs = pd.Series(p_diffs)\n",
    "p_diffs.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`g)` What proportion of the **p_diffs** are greater than the difference observed between `treatment` and `control` in `df`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "treatment_conversion_rate = df.query('group == \"treatment\"')['converted'].mean()\n",
    "control_conversion_rate = df.query('group == \"control\"')['converted'].mean()\n",
    "\n",
    "observed_diff = treatment_conversion_rate - control_conversion_rate\n",
    "# Proportion of p_diffs greater than the observed difference\n",
    "proportion_greater = (p_diffs > observed_diff).mean()\n",
    "print(\"Observed Difference:\", observed_diff)\n",
    "print(\"Proportion Greater Than Observed:\", proportion_greater)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`h)` In words, explain what you just computed in part `g)`  What is this value called in scientific studies?  What does this value mean in terms of whether or not there is a difference between the new and old pages using our Type I error rate of 0.05?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer Here**\n",
    "- **What was computed:**\n",
    "    - the **observed difference** of **0.0501** represents the actual difference in conversion rates between the treatment group and the control group based on the dataset.\n",
    "    - The **proportion greater than observed** (p-value) of **0.0** represents the proportion of simulated differences under the null hypothesis that are greater than or equal to the observed difference.\n",
    "- **What is this value called in scientific studies?**\n",
    "    - The proportion greater than observed is called the **p-value** in scientific studies.\n",
    "- **What does this value mean?**\n",
    "    - A p-value of **0.0** means that the probability of observing a difference as extreme as 0.0501 (or more extreme) under the null hypothesis (no true difference) is practically zero.\n",
    "    - Since the p-value is less than our Type I error rate (*α*=0.05), we reject the null hypothesis.\n",
    "- **Conclusion:**\n",
    "    - There is strong statistical evidence to suggest that the conversion rate for the new page (treatment) is significantly higher than the old page (control) at the 5% significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='algorithms'></a>\n",
    "### Part IV - Algorithms\n",
    "\n",
    "`1.` In this final part, you will see that the result you acheived in the previous A/B test can also be acheived by performing regression.  All the code needed for the modeling and results of the modeling for sections `b) - f)` have been completed for you. \n",
    "\n",
    "**You will need to complete sections `a)` and `g)`.**  \n",
    "\n",
    "**Then use the code from `1.` to assist with the question `2.`   You should be able to modify the code to assist in answering each of question 2's parts.**<br><br>\n",
    "\n",
    "`a)` Since each row is either a conversion or no conversion, what type of regression should you be performing in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer Here**\n",
    "\n",
    "Since each row represents a binary outcome (conversion = 1, no conversion = 0), the appropriate type of regression to perform is **logistic regression**.\n",
    "\n",
    "#### Why Logistic Regression?\n",
    " - Logistic regression is designed for binary classification problems where the dependent variable is categorical (e.g., 0 or 1).\n",
    " - It estimates the probability of the dependent variable being 1, given the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to use **statsmodels** to fit the regression model you specified in part `a)` to see if there is a significant difference in conversion based on which page a customer receives.  \n",
    "\n",
    "`b)` However, you first need to create a column for the intercept, and create a dummy variable column for which page each user received.  Add an **intercept** column, as well as an **ab_page** column, which is 1 when an individual receives the **treatment** and 0 if **control**.\n",
    "\n",
    "It may be helpful to look at the [get_dummies documentation](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) to encode the `ab_page` column.\n",
    "\n",
    "Below you can see an example of the new columns that will need to be added (The order of columns is not important.): **This question is completed for you**\n",
    "\n",
    "##### Example DataFrame\n",
    "| intercept   | group       | ab_page     | converted   |\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| 1           |  control    |  0          |  0          |\n",
    "| 1           |  treatment  |  1          |  0          |\n",
    "| 1           |  treatment  |  1          |  0          |\n",
    "| 1           |  control    |  0          |  0          |\n",
    "| 1           |  treatment  |  1          |  1          |\n",
    "| 1           |  treatment  |  1          |  1          |\n",
    "| 1           |  treatment  |  1          |  0          |\n",
    "| 1           |  control    |  0          |  1          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['intercept'] = 1\n",
    "df['ab_page'] = pd.get_dummies(df['group'], drop_first=True)['treatment']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`c)`  Create your `X` matrix and `y` response column that will be passed to your model, where you are testing if there is a difference in `treatment` vs. `control`. **This question is completed for you**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure all columns are numeric\n",
    "X = df[['intercept', 'ab_page']].astype(float)\n",
    "y = df['converted'].astype(float)\n",
    "X.head(),y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d)` Use **statsmodels** to import and fit your regression model on the `X` and `y` from part `c)`. \n",
    "\n",
    "You can find the [statsmodels documentation to assist with this exercise here](https://www.statsmodels.org/stable/discretemod.html).  **This question is completed for you**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Logit Model\n",
    "logit_mod = sm.Logit(y, X)\n",
    "logit_res = logit_mod.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`e)` Provide the summary of your model below. **This question is completed for you**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(logit_res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f)` What is the p-value associated with **ab_page**? Does it lead you to the same conclusion you drew in the **Experiment** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer Here.**\n",
    "- **P-Value for** `ab_page`:\n",
    "    - The p-value associated with the `ab_page` coefficient is **0.000** (or 1.81×10^−86)\n",
    "    - This extremely small p-value indicates strong evidence against the null hypothesis for the `ab_page` variable.\n",
    "\n",
    "**Does it Lead to the Same Conclusion?**\n",
    "\n",
    "Yes, this result supports the same conclusion drawn in the **Experiment** section:\n",
    "\n",
    "- The null hypothesis assumes no difference between the treatment and control groups in terms of conversion rate.\n",
    "- The p-value of **0.000** indicates that the difference in conversion rates between the treatment and control groups is statistically significant at any reasonable Type I error threshold (e.g., *𝛼* = 0.05).\n",
    "\n",
    "**Final Conclusion:**\n",
    "The logistic regression analysis confirms that the **treatment group (new page)** leads to a significantly higher conversion rate compared to the **control group (old page)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2. a)` Now you will want to create two new columns as dummy variables for `US` and `UK`.  Again, use `get_dummies` to add these columns.  The dataframe you create should include at least the following columns (If both columns for `US` and `UK` are `0` this represents `CA`.  The order of rows and columns is not important for you to match - it is just to illustrate how columns should connect to one another.):\n",
    "\n",
    "##### Example DataFrame\n",
    "| intercept   | group       | ab_page     | converted   | country     |  US         | UK          |\n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| 1           |  control    |  0          |  0          |  US         |  1          |  0          |\n",
    "| 1           |  treatment  |  1          |  0          |  UK         |  0          |  1          |\n",
    "| 1           |  treatment  |  1          |  0          |  US         |  1          |  0          |\n",
    "| 1           |  control    |  0          |  0          |  US         |  1          |  0          |\n",
    "| 1           |  treatment  |  1          |  1          |  CA         |  0          |  0          |\n",
    "| 1           |  treatment  |  1          |  1          |  UK         |  0          |  1          |\n",
    "| 1           |  treatment  |  1          |  0          |  US         |  1          |  0          |\n",
    "| 1           |  control    |  0          |  1          |  US         |  1          |  0          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "df = pd.read_csv('ab_data.csv')\n",
    "\n",
    "# Add intercept column\n",
    "df['intercept'] = 1\n",
    "\n",
    "# Add ab_page column\n",
    "df['ab_page'] = pd.get_dummies(df['group'], drop_first=True)\n",
    "\n",
    "# Add dummy variables for 'US' and 'UK' with binary values\n",
    "country_dummies = pd.get_dummies(df['country'], drop_first=True).astype(int)\n",
    "df = pd.concat([df, country_dummies], axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`b)`  Create your `X` matrix and `y` response column that will be passed to your model, where you are testing if there is 6\n",
    "* a difference in `converted` between `treatment` vs. `control`\n",
    "* a difference in `converted` between `US`, `UK`, and `CA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the X matrix and y response column for logistic regression\n",
    "\n",
    "# Define X to include intercept, ab_page, and country dummies (US and UK)\n",
    "newX = df[['intercept', 'ab_page', 'US', 'UK']].astype(float)\n",
    "\n",
    "# Define y as the response variable\n",
    "newy = df['converted'].astype(float)\n",
    "\n",
    "# Display X and y to verify\n",
    "newX, newy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`c)` Use **statsmodels** to import and fit your regression model on the `X` and `y` from part `b)`. \n",
    "You can find the [statsmodels documentation to assist with this exercise here](https://www.statsmodels.org/stable/discretemod.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit the logistic regression model\n",
    "log_model = sm.Logit(newy, newX)\n",
    "results = log_model.fit()\n",
    "\n",
    "# Display the summary of the results\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d)` Provide the summary of your model below.\n",
    "\n",
    "### Summary of the Logistic Regression Model:\n",
    "#### Key Observations:\n",
    "1. **Dependent Variable**\n",
    "    - The `converted column` is the binary response variable indicating whether a user converted (1) or not (0).\n",
    "2. **Independent Variables**\n",
    "    - Intercept: The baseline conversion rate when all other predictors are 0.\n",
    "    - ab_page: The difference in conversion rates between the treatment and control groups.\n",
    "    - US: The difference in conversion rates for users from the US compared to the baseline (CA).\n",
    "    - UK: The difference in conversion rates for users from the UK compared to the baseline (CA).\n",
    "3. **Coefficients**:\n",
    "    - **Intercept**: -2.1930\n",
    "        - This corresponds to the baseline log-odds of conversion.\n",
    "    - **ab_page**: 0.4466\n",
    "        - A positive and significant coefficient indicates that being in the treatment group increases the log-odds of conversion compared to the control group.\n",
    "    - **US**: 0.0727\n",
    "        - A positive coefficient but with a p-value of 0.170, which is not statistically significant (p > 0.05).\n",
    "    - **UK**: 0.0067\n",
    "        - A positive coefficient but with a p-value of 0.905, which is not statistically significant (p > 0.05).\n",
    "4. **Statistical Significance**:     \n",
    "    - **ab_page**: The p-value is 0.000, indicating a statistically significant effect of being in the treatment group.\n",
    "    - **US and UK**: Both p-values are greater than 0.05, indicating that the country of the user (US or UK) does not have a statistically significant effect on conversion compared to the baseline (CA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`e)` What do the `p-values` associated with `US` and `UK` suggest in relation to how they impact `converted`? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of P-values for US and UK**:\n",
    "#### P-values:\n",
    "- **US**: 𝑝 = 0.170\n",
    "- **UK**: 𝑝 = 0.905\n",
    "#### What do these P-values suggest?\n",
    "1. **US**:\n",
    "    - The p-value for the US coefficient is 0.170, which is greater than the common significance level (𝛼 = 0.05).\n",
    "    - This means that there is **no statistically significant evidence** that being from the US impacts the likelihood of conversion compared to the baseline country (CA).\n",
    "2. **UK**:\n",
    "    - The p-value for the `UK` coefficient is 0.905, which is much greater than 𝛼 = 0.05.\n",
    "    - This indicates that being from the UK also does **not significantly impact** the likelihood of conversion compared to the baseline (CA).\n",
    "\n",
    "### **Conclusion**:\n",
    "\n",
    "The results suggest that the **country of origin (US or UK)** does not play a significant role in determining whether a user converts, relative to the baseline country (CA). Any differences in conversion rates across these countries are likely due to random variation rather than a true effect.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PowerPoint data below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Was The Experiment Implemented?​ slide\n",
    "\n",
    "#Total Variant Visitors: [The number of variant visitors]\n",
    "#Total Control Participants:​ [The number of control visitors]\n",
    "# Compute total visitors\n",
    "total_variant_visitors = df.query('group == \"treatment\"').shape[0]\n",
    "total_control_visitors = df.query('group == \"control\"').shape[0]\n",
    "\n",
    "# Display results\n",
    "print(\"Total Variant Visitors:\", total_variant_visitors)\n",
    "print(\"Total Control Visitors:\", total_control_visitors)\n",
    "\n",
    "#[Update the Below Chart With Your Chart of Where Users Are From]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
